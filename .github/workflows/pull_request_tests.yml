name: Run tests

on: pull_request

jobs:
  pytest:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest]
        python-version: ["3.11"]
        include:
          # Installing Spark on a Windows VM needs work.
          - os: ubuntu-latest
            python-version: "3.11"

    steps:
    - uses: actions/checkout@v4
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        python -m pip install maturin
        python -m pip install '.[dev,spark]' --group=pyhive
        wget https://archive.apache.org/dist/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz
        tar -xzf spark-4.0.0-bin-hadoop3.tgz
        export SPARK_HOME=$(pwd)/spark-4.0.0-bin-hadoop3
        ${SPARK_HOME}/sbin/start-thriftserver.sh
    - name: Run tests
      run: |
        # This does not run the AWS tests.
        # Credentials would need to be configured.
        git submodule init
        git submodule update
        DSGRID_BACKEND_ENGINE=duckdb python -m pytest -v --disable-warnings --cov=./ --cov-report=xml:coverage.xml tests dsgrid/utils/py_expression_eval/tests.py
        # Restart from scratch.
        rm -rf tests/data/registry
        DSGRID_BACKEND_ENGINE=spark python -m pytest -v --disable-warnings --cov=./ --cov-append --cov-report=xml:coverage.xml tests
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4.2.0
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        name: codecov-umbrella
        fail_ci_if_error: false
        verbose: true

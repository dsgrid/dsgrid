name: Run tests

on: pull_request

jobs:
  pytest:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        include:
          - os: ubuntu-latest
            python-version: "3.13"
          - os: windows-latest
            python-version: "3.13"

    steps:
    - uses: actions/checkout@v6
      with:
        submodules: true

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache Spark
      if: runner.os == 'Linux'
      id: cache-spark
      uses: actions/cache@v4
      with:
        path: spark-4.0.0-bin-hadoop3
        key: spark-4.0.0-bin-hadoop3

    - name: Install dependencies (Linux)
      if: runner.os == 'Linux'
      run: |
        python -m venv .venv
        source .venv/bin/activate
        pip install --upgrade pip
        pip install maturin
        pip install -e '.[dev,spark]'

        # Only download if cache miss
        if [ ! -d "spark-4.0.0-bin-hadoop3" ]; then
          echo "Spark cache missed, downloading..."
          wget -q https://archive.apache.org/dist/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz
          tar -xzf spark-4.0.0-bin-hadoop3.tgz
        else
          echo "Spark restored from cache."
        fi

        export SPARK_HOME=$(pwd)/spark-4.0.0-bin-hadoop3
        ${SPARK_HOME}/sbin/start-thriftserver.sh
        # Wait for the Thrift server to be ready
        for i in $(seq 1 30); do
          if nc -z localhost 10000 2>/dev/null; then
            echo "Thrift server is ready."
            break
          fi
          echo "Waiting for Thrift server... ($i/30)"
          sleep 2
        done

    - name: Install dependencies (Windows)
      if: runner.os == 'Windows'
      run: |
        python -m venv .venv
        .venv\Scripts\Activate.ps1
        pip install --upgrade pip
        pip install maturin
        pip install -e '.[dev]'

    - name: Run tests (Linux)
      if: runner.os == 'Linux'
      run: |
        # This does not run the AWS tests.
        # Credentials would need to be configured.
        source .venv/bin/activate
        export SPARK_HOME=$(pwd)/spark-4.0.0-bin-hadoop3
        DSGRID_BACKEND_ENGINE=duckdb python -m pytest -v --disable-warnings --cov=./ --cov-report=xml:coverage.xml tests dsgrid/utils/py_expression_eval/tests.py
        # Restart from scratch.
        rm -rf tests/data/registry
        DSGRID_BACKEND_ENGINE=spark python -m pytest -v --disable-warnings --cov=./ --cov-append --cov-report=xml:coverage.xml tests

    - name: Run tests (Windows)
      if: runner.os == 'Windows'
      run: |
        # This does not run the AWS tests.
        # Credentials would need to be configured.
        .venv\Scripts\Activate.ps1
        $env:DSGRID_BACKEND_ENGINE = 'duckdb'
        python -m pytest -v --disable-warnings --cov=./ --cov-append --cov-report=xml:coverage.xml tests dsgrid/utils/py_expression_eval/tests.py

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4.2.0
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        flags: ${{ runner.os }}
        name: ${{ matrix.os }}
        fail_ci_if_error: false
        verbose: true

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerate `load_data_lookup` for EFS-ComStock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from dsgrid.utils.spark import init_spark\n",
    "from dsgrid.utils.spark_partition import SparkPartition\n",
    "from enumerate_load_table_lookup import EnumerateTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.debug(\"DSG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spark\n",
    "spark = init_spark(\"dsgrid-load\")  # <--- same as a pyspark instance in shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize class func\n",
    "partition = SparkPartition()\n",
    "enumeration = EnumerateTable()\n",
    "\n",
    "# define location to load_data_lookup\n",
    "lookup_file = (\n",
    "    \"/Users/lliu2/Documents/dsGrid/dsgrid_v2.0.0/commercial/load_data_lookup.parquet\"  # <---\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the original data\n",
    "relocated_file = enumeration.relocate_original_file(lookup_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check load_data_lookup_orig has been created:\n",
    "os.listdir(os.path.dirname(lookup_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data as a Spark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data from relocated file\n",
    "df_lookup = spark.read.parquet(relocated_file)\n",
    "\n",
    "df_lookup.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get keys to enumerate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_exclude = [\"scale_factor\", \"id\"]\n",
    "keys = [x for x in df_lookup.columns if x not in keys_to_exclude]\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Expand Load_data_lookup to all combinations of keys, keep new combinations null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup_full = enumeration.enumerate_lookup_by_keys(df_lookup, keys)\n",
    "\n",
    "df_lookup_full.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Check\n",
    "#### 4.1. Mapping Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumeration.enumeration_report(df_lookup_full, df_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Assertion checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) set of (data) id is the same before and after enumeration\n",
    "# 2) make sure N_df_lookup_full is the product of the length of each key\n",
    "enumeration.assertion_checks(df_lookup_full, df_lookup, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. check partitioning choices and *optimal* # of sharded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup_full.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = df_lookup_full.columns\n",
    "\n",
    "partition_stats = []\n",
    "for key in df_cols:\n",
    "    report = partition.file_size_if_partition_by(df_lookup_full, key)\n",
    "    partition_stats.append(pd.DataFrame(report))\n",
    "\n",
    "partition_stats = pd.concat(partition_stats, axis=1)\n",
    "\n",
    "partition_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *optimal* # of files\n",
    "n_files = partition.get_optimal_number_of_files(df_lookup_full)\n",
    "n_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- `write.partitionBy('col1','col2',...)`: export partitions by creating hierarchical subfolders (e.g., col1=0/col2=0/col3=.../part-0)\n",
    "- `write.option(\"maxRecordsPerFile\", n).partitionBy(col)`: use to control # of unique records (to n) per partition\n",
    "- `coalesce(n).write`: combine into n partitions without shuffling, will not go larger than # of RDD files (spark default is 200)\n",
    "- `repartition(n).write`: try to evenly distribute, if n > # of unique rows, some partitions will be empty\n",
    "- `repartition(col).write`: create partitions by unique col field, 1 empty/very small partition will be created in addition to # of unique col records\n",
    "- `repartition(n, col).write`: # files exported = min(n, # of unique fields for col)\n",
    "- `repartition(n).write.partitionBy(col)`: create subfolder by unique col fields, each subfolder contains n partitions\n",
    "- `write.partitionBy(col1).bucketBy(n_buckets, col2)`: distribute partitions into smaller pieces called buckets, col2 can not be the same as col1, good for reducing shuffles/exchanges when tables get joined, # of files exported = n_unique_fields_in_col1 x n_buckets x n_repartitions (if applicable)\n",
    "\n",
    "File format: part-[partiton#]-[bucket#]...snappy.parquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "`df_lookup_full.repartition(3).write \\\n",
    "    .partitionBy(\"sector\") \\\n",
    "    .bucketBy(2, \"subsector\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", lookup_file)\\\n",
    "    .saveAsTable(\"load_data_lookup\", format='parquet')`\n",
    "    \n",
    "Outputs:\n",
    "\n",
    "```\n",
    "load_data_lookup.parquet\n",
    "├── _SUCCESS\n",
    "├── sector=com\n",
    "│   ├── part-00000-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "│   ├── part-00000-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "│   ├── part-00001-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "│   ├── part-00001-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "│   ├── part-00002-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "│   └── part-00002-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "└── sector=res\n",
    "    ├── part-00000-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "    ├── part-00000-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "    ├── part-00001-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "    ├── part-00001-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "    ├── part-00002-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "    └── part-00002-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "\n",
    "2 directories (controlled by `partitionBy`), 13 files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumeration.save_file(df_lookup_full, lookup_file, n_files, repartition_by=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
